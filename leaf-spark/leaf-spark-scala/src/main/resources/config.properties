
spark
export JAVA_HOME=/usr/local/jdk1.7   #Java环境变量
export SCALA_HOME=/usr/local/scala-2.10.6 #SCALA环境变量


spark-env.sh
export SPARK_MASTER_IP=hadoop202
export HADOOP_HOME=/opt/hadoop-2.7.3  
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop

bin/spark-shell --master=yarn --deploy-mode=client

bin/spark-shell --master yarn --deploy-mode cluster 


hadoop

　查看hdfs底下包含的文件目录
hadoop dfs -ls /

　　hdfs创建一个文件目录
hdfs dfs -mkdir /input

hadoop fs -mkdir /tmp/input              在HDFS上新建文件夹
hadoop fs -put input1.txt /tmp/input  把本地文件input1.txt传到HDFS的/tmp/input目录下
hadoop fs -get  input1.txt /tmp/input/input1.txt  把HDFS文件拉到本地
hadoop fs -ls /tmp/output                  列出HDFS的某目录
hadoop fs -cat /tmp/ouput/output1.txt  查看HDFS上的文件
hadoop fs -rmr /home/less/hadoop/tmp/output  删除HDFS上的目录
hadoop dfsadmin -report 查看HDFS状态，比如有哪些datanode，每个datanode的情况
hadoop dfsadmin -safemode leave  离开安全模式
hadoop dfsadmin -safemode enter  进入安全模式


bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar wordcount /tmp/input /output1